{
  "hash": "8da29b132eabadef1d4c388673a8bf59",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Importing data\"\nformat: html\n---\n\n\n\n\n## Packages for data import\n  \nThere is a wide range of R packages for importing all different data formats to R. R also has its own data format called \".rds\". Datasets can be written and read respectively via the R functions `saveRDS()` and `readRDS()`. However, you will rarely find data saved in rds format on a webpage where you can download it. R has a package for importing almost any data format, and if in doubt, ask your favorite chatbot for help. The sections below go through the most common data formats.\n\n### Delimited files (`readr`)\n\nThe package [readr](https://readr.tidyverse.org/), which is also part of the [tidyverse](https://www.tidyverse.org/), has a lot of functions to deal with CSV files and other delimited files. The following code shows examples of the most commonly used functions from the `readr` package. Note the difference between `read_csv()` (comma delimiter) and `read_csv2()` (semicolon delimiter).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readr)\n\n# Read CSV files, comma separation\ndata <- read_csv(\"file.csv\")\n\n# Read files with semicolon separation (common in European data)\ndata <- read_csv2(\"file.csv\")\n\n# Read tab-separated files\ndata <- read_tsv(\"file.tsv\")\n\n# Read files with custom delimiters, here a pipe: \"|\"\ndata <- read_delim(\"file.txt\", delim = \"|\")\n```\n:::\n\n\n\n\nThe `readr` functions automatically detect column types and handle common issues like missing values. You can specify column types explicitly using the `col_types` argument:\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(\"file.csv\", \n                 col_types = cols(\n                   id = col_integer(),\n                   name = col_character(),\n                   date = col_date(),\n                   value = col_double()\n                 ))\n```\n:::\n\n\n\n\n### Excel files (`readxl`)\n\nFor Excel files (.xlsx and .xls), the [readxl](https://readxl.tidyverse.org/) package can be used and is also part of the tidyverse ecosystem. There are a few alternatives as well: the [`xlsx` package](https://colearendt.github.io/xlsx/index.html) and the [`openxlsx` package](https://ycphs.github.io/openxlsx/index.html#openxlsx-). If you have to write data to Excel or load formatted sheets, it might be relevant to look at especially `openxlsx`, but for reading simple Excel files, `readxl` should do the trick.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(readxl)\n\n# Read the first sheet\ndata <- read_excel(\"file.xlsx\")\n\n# Read a specific sheet by name or number\ndata <- read_excel(\"file.xlsx\", sheet = \"Sheet2\")\ndata <- read_excel(\"file.xlsx\", sheet = 2)\n\n# Read a specific range\ndata <- read_excel(\"file.xlsx\", range = \"A1:D10\")\n\n# Skip rows (useful for files with headers or metadata)\ndata <- read_excel(\"file.xlsx\", skip = 3)\n```\n:::\n\n\n\n\nYou can also list all sheet names in an Excel file:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexcel_sheets(\"file.xlsx\")\n```\n:::\n\n\n\n\n### Statistical software formats (`haven`)\n\nThe [haven](https://haven.tidyverse.org/) package imports data from SPSS, Stata, and SAS.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(haven)\n\n# Stata files\ndata <- read_dta(\"file.dta\")\n\n# SAS files\ndata <- read_sas(\"file.sas7bdat\")\n\n# SPSS files\ndata <- read_sav(\"file.sav\")\n```\n:::\n\n\n\n\nThese functions preserve variable labels and value labels from the original statistical software. This can also make the datasets time-consuming to import. The `n_max` argument can be used to only import part of a dataset. Afterwards, the `col_select` argument can be used to read only the necessary columns. The following code shows an example of this workflow.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Reading only first observation to inspect columns of the data\ndata_obs1 <- read_dta(\"file.dta\", n_max = 1)\n\n# Reading the necessary columns\ndata <- read_dta(\"file.dta\", col_select = c(\"id\", \"age\", \"sex\", \"education\", \"income\", \"district\"))\n```\n:::\n\n\n\n\n### JSON files (`jsonlite`)\n\nJSON (JavaScript Object Notation) is increasingly common for data exchange, especially from APIs. The [jsonlite](https://arxiv.org/abs/1403.2805) package handles JSON data efficiently.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(jsonlite)\n\n# Read JSON from file\ndata <- fromJSON(\"file.json\")\n\n# Read JSON from URL\ndata <- fromJSON(\"https://api.example.com/data.json\")\n\n# Convert R object to JSON\njson_string <- toJSON(data, pretty = TRUE)\n```\n:::\n\n\n\n\n\n### Large files (`data.table` and `vroom`)\n\nFor very large files, specialized packages offer better performance, especially the `data.table` and `vroom` packages.\n  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(vroom)\n\n# data.table's fast reader\ndata <- fread(\"large_file.csv\")\n\n# vroom for very fast reading\ndata <- vroom(\"large_file.csv\")\n```\n:::\n\n\n\n\n## Reproducible paths and R projects\n\nOne of the most common issues when sharing R code or moving projects between computers is broken file paths. Using R Projects and the here package creates reproducible workflows that work across different operating systems and directory structures. For this reason, it is recommended to have an R project when working on your code.\n\n### R Projects\n\nR Projects (.Rproj files) create a self-contained workspace for your analysis. When opening an R Project, RStudio automatically sets the working directory to the project folder. This means you can use relative paths that work regardless of where the project is stored on your computer. Furthermore, you can export your folder structure and send it to another person, who will be able to run the same code through the R Project.\n\nTo create an R Project:\n\n  1. File → New Project → New Directory → New Project\n  2. Give your project a name and choose a location\n  3. RStudio will create a .Rproj file and set up the folder structure\n\n### The `here` package\n\nThe here package builds file paths relative to your project root, and therefore makes sense to use within your project. This also makes your code more portable and robust.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(here)\n\n# Instead of this (brittle, won't work on other computers):\ndata <- read_csv(\"C:/Users/YourName/Documents/my_project/data/file.csv\")\n\n# Use this (portable, works anywhere):\ndata <- read_csv(here(\"data\", \"file.csv\"))\n\n# The here() function builds the complete path\nhere(\"data\", \"file.csv\")\n# Returns: \"/path/to/your/project/data/file.csv\"\n```\n:::",
    "supporting": [
      "import_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}