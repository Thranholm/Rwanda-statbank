{
  "hash": "598b6b0e422415f35baf8732d3cc06d9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Large PX-files and data\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n## Size matters\n\nPX-files can contain a lot of data, which is great. However this also means that somewhere along the process it will require handling of this large data. The chapter [Importing data](import_data.qmd) gives some tips about handling large datasets via functions in the `haven` package with `n_max` and `col_select`. The same arguments are also available for csv-files e.g. in the function `readr::read_csv()`.\n\n### Parquet files and rds-files\n\nAn efficient file format in terms of file size is the parquet format. To exemplify this we retrieve a large PX-table from [Statistics Sweden's Statistical Database](https://www.statistikdatabasen.scb.se/pxweb/en/ssd/START__LE__LE0105__LE0105C/LE0105Demogr02/). We have found a large demography table and imported that through the functions in the `pxweb` package, which helps dealing with PX-web API. Afterwards, we use `pxmake`'s `px()` function to get it as a PX-object in R.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\" code-summary=\"Get PX-table from API\"}\nlibrary(pxmake)\nlibrary(tidyverse)\nlibrary(pxweb)\n\nfile_api <- pxweb_get_data(url = \"https://api.scb.se/OV0104/v1/doris/en/ssd/START/LE/LE0105/LE0105C/LE0105Demogr02\", query = '{\n  \"query\": [\n    {\n      \"code\": \"Hushallsstallning\",\n      \"selection\": {\n        \"filter\": \"item\",\n        \"values\": [\n          \"ABarn\",\n          \"aensf\",\n          \"Aensm\",\n          \"Asamm\",\n          \"Agift\",\n          \"Abild\",\n          \"Aovri\",\n          \"Apers\"\n        ]\n      }\n    },\n    {\n      \"code\": \"Fodelseregion\",\n      \"selection\": {\n        \"filter\": \"item\",\n        \"values\": [\n          \"200\",\n          \"020\",\n          \"030\",\n          \"040\",\n          \"050\",\n          \"010\",\n          \"100\"\n        ]\n      }\n    },\n    {\n      \"code\": \"Vistelsetid\",\n      \"selection\": {\n        \"filter\": \"item\",\n        \"values\": [\n          \"TOT\",\n          \"INRF\",\n          \"0-3\",\n          \"4-9\",\n          \"10-\",\n          \"US\"\n        ]\n      }\n    },\n    {\n      \"code\": \"Alder\",\n      \"selection\": {\n        \"filter\": \"vs:ÅlderLE0105fem\",\n        \"values\": [\n          \"-4\",\n          \"5-9\",\n          \"10-14\",\n          \"15-19\",\n          \"20-24\",\n          \"25-29\",\n          \"30-34\",\n          \"35-39\",\n          \"40-44\",\n          \"45-49\",\n          \"50-54\",\n          \"55-59\",\n          \"60-64\",\n          \"65-69\",\n          \"70-74\",\n          \"75-79\",\n          \"80-84\",\n          \"85-89\",\n          \"90-94\",\n          \"95-99\",\n          \"100+\"\n        ]\n      }\n    },\n    {\n      \"code\": \"ContentsCode\",\n      \"selection\": {\n        \"filter\": \"item\",\n        \"values\": [\n          \"000004SM\"\n        ]\n      }\n    },\n    {\n      \"code\": \"Tid\",\n      \"selection\": {\n        \"filter\": \"item\",\n        \"values\": [\n          \"2011\",\n          \"2012\",\n          \"2013\",\n          \"2014\",\n          \"2015\",\n          \"2016\",\n          \"2017\",\n          \"2018\",\n          \"2019\",\n          \"2020\",\n          \"2021\",\n          \"2022\",\n          \"2023\"\n        ]\n      }\n    }\n  ],\n  \"response\": {\n    \"format\": \"px\"\n  }\n}')\n\nfile_api2 <- file_api %>% \n  pivot_longer(cols = everything())\n\n# getting one of the two files collected in batches as px\npx_api <- px(file_api2$value[[1]])\n\npx_from_api <- map(file_api2$value, px) %>% \n  map(px_data) %>% \n  bind_rows() %>% \n  px_data(px_api, .)\n```\n:::\n\n\n\n\nLet's just view the data. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npx_from_api %>% \n  px_data()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 275,184 × 8\n   Hushallsstallning Fodelseregion Vistelsetid Kon   Alder ContentsCode Tid  \n   <chr>             <chr>         <chr>       <chr> <chr> <chr>        <chr>\n 1 ABarn             200           TOT         TOT2  -4    000004SM     2011 \n 2 ABarn             200           TOT         TOT2  -4    000004SM     2012 \n 3 ABarn             200           TOT         TOT2  -4    000004SM     2013 \n 4 ABarn             200           TOT         TOT2  -4    000004SM     2014 \n 5 ABarn             200           TOT         TOT2  -4    000004SM     2015 \n 6 ABarn             200           TOT         TOT2  -4    000004SM     2016 \n 7 ABarn             200           TOT         TOT2  -4    000004SM     2017 \n 8 ABarn             200           TOT         TOT2  5-9   000004SM     2011 \n 9 ABarn             200           TOT         TOT2  5-9   000004SM     2012 \n10 ABarn             200           TOT         TOT2  5-9   000004SM     2013 \n# ℹ 275,174 more rows\n# ℹ 1 more variable: figures_ <dbl>\n```\n\n\n:::\n:::\n\n\n\n\nThe dataset has 275184 rows and 7 explanatory variables and one frequency variable.\n\nThe `px_save()` function also allows for saving data in Excel, rds or parquet. This can be useful in different situations. The Excel-format is not very efficient for storing large datasets in terms of file size, so it will not be the focus for this section. However, it can be useful for other cases, see for example the chapter about [Multiple languages in PX-files](px_multi_lingual.px).\n\nUsing the `px_save` function, we can easily save the data as both rds and parquet format using the `data_path` argument.\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npx_save(px_from_api, \"px_api.R\", \n        data_path = \"px_api.rds\")\n\npx_save(px_from_api, \"px_api2.R\",\n        data_path = \"px_api.parquet\")\n```\n:::\n\n\n\n\nNote that when using the `data_path` argument to save to parquet or rds, we also save an R-script. The R-script contains the code to create the PX-object associated with the dataset.\n\nThe rds-format is R's data format and works fairly quick with R to read and write files. However the alternative, parquet files might sometimes be even better in terms of speed (read/write) and size.\n\nIn the case above, the dataset, even though it has many rows, is not necessarily a large dataset. Sometimes a dataset could have millions of rows if it for example covers population data for a country.\n\n## Splitting data and efficient packages\n\nWhen working with datasets containing millions or tens of millions of observations, traditional R approaches can quickly become memory-intensive and computationally slow. \n\nHere we can instead use two strategies: splitting the dataset into smaller chunks and using packages with the specific focus of working with large datasets in R. These chunks could for instance be each year or a similar division of the data, where you can run your code in meaningful chunks.\n\nFurthermore, it can be advised to take advantage of R packages with a specific focus on working with large data. Some of the most common packages are `data.table`, `dplyr` with `dtplyr`, `arrow`, and `vroom`.\n\nSometimes we might have a large external file, like a big Stata dataset, which can be time-consuming for R to read. Then we can use the arguments `skip` and `n_max` to read in the data in chunks, combine it and then write to an rds-file so it is easier to read that data to R in the future.\n\nThe code below does just that: it takes a large Stata file, reads it in chunks of 100000 observations (can be adjusted) and saves a combined rds-file.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(haven)\nlibrary(data.table)\n\n# Set chunks to write to and initial values\nchunks <- list()\nskip_rows <- 0\ni <- 1\n\nrepeat {\n  chunk <- read_stata(\"large_set.dta\", \n                      skip = skip_rows, \n                      n_max = 100000)\n  if (nrow(chunk) == 0) break\n  \n  chunks[[i]] <- chunk\n  skip_rows <- skip_rows + 100000\n  i <- i + 1\n  \n  if (nrow(chunk) < 100000) break\n}\n\n# Combine and save\ncombined_data <- rbindlist(chunks)\nsaveRDS(combined_data, \"data_test.rds\")\n```\n:::",
    "supporting": [
      "px_large_data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}